## Project Overview
This project focuses on distributed training of models, optimizing performance through techniques such as autocasting to bfloat16, kernel fusion, and flash attention. It delves into GPU architecture to understand how tensors are processed efficiently. By leveraging distributed training on large raw datasets and utilizing **tiktoken** as the tokenizer, the project aims to develop a robust base model with enhanced computational efficiency and scalability.

## References

Flash_Attention https://arxiv.org/pdf/2205.14135

A100_GPU https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf

